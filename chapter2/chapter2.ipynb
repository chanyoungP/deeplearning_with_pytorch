{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca8baa6e",
   "metadata": {},
   "source": [
    "## 딥러닝 파이토치 교과서 \n",
    "### chapter 2  \n",
    "#### 파이토치 기초 문법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37df3362",
   "metadata": {},
   "source": [
    "#### 텐서 생성 및 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "336fd14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "tensor([[1, 2],\n",
      "        [3, 4]], device='cuda:0')\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]], dtype=torch.float64)\n",
      "[[1 2]\n",
      " [3 4]]\n",
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.tensor([[1,2],[3,4]])) # 2차원 형태의 텐서 생성 \n",
    "print(torch.tensor([[1,2],[3,4]], device = \"cuda:0\")) # gpu에 텐서 생성 \n",
    "print(torch.tensor([[1,2],[3,4]],dtype = torch.float64)) # dtype을 이용하여 텐서 생성\n",
    "\n",
    "temp = torch.tensor([[1,2],[3,4]])\n",
    "print(temp.numpy()) # tensor를 넘파이 배열로 변환 \n",
    "\n",
    "temp = torch.tensor([[1,2],[3,4]],device = \"cuda:0\")\n",
    "print(temp.to(\"cpu\").numpy()) #gpu상의 텐서를 cpu의 텐서로 변환한 후 넘파이 배열로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c86079",
   "metadata": {},
   "source": [
    "#### 텐서의 인덱스 조작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "918e92be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.) tensor(2.) tensor(7.)\n",
      "--------------------\n",
      "tensor([3., 4., 5.]) tensor([5., 6.])\n"
     ]
    }
   ],
   "source": [
    "temp = torch.FloatTensor([1,2,3,4,5,6,7]) #1차원 벡터 생성\n",
    "print(temp[0],temp[1],temp[-1]) # 인덱스로 접근 \n",
    "print(\"--------------------\") \n",
    "print(temp[2:5],temp[4:-1]) #슬라이스로 접근 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d78c23",
   "metadata": {},
   "source": [
    "#### 텐서 연산 및 차원 조작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09f9353e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([1,2,3]) \n",
    "w = torch.tensor([3,4,6]) \n",
    "\n",
    "print(w - v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0367b5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "----------------\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4]])\n",
      "----------------\n",
      "tensor([1, 2, 3, 4])\n",
      "----------------\n",
      "tensor([[1, 2, 3, 4]])\n",
      "----------------\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4]])\n"
     ]
    }
   ],
   "source": [
    "#차원 조작\n",
    "temp = torch.tensor([\n",
    "    [1,2],\n",
    "    [3,4]\n",
    "])\n",
    "print(temp.shape)\n",
    "print(\"----------------\")\n",
    "print(temp.view(4,1)) # 4x1 로 변환 \n",
    "print(\"----------------\")\n",
    "print(temp.view(-1)) # 1차원 벡터로 변형\n",
    "print(\"----------------\")\n",
    "print(temp.view(1,-1)) # -1은 (1,?)과 같은 의미로 다른 차원으로부터 해당 값을 유추하겠다는 것 여기서는 (1,4)로 유추\n",
    "print(\"----------------\")\n",
    "print(temp.view(-1,1)) # 마찬가지로 (?,1)과 같은 의미로 -1에 해당하는 차원은 유추하여 (4,1)로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8512ffae",
   "metadata": {},
   "source": [
    "### 2.2.2 데이터 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2581b3a5",
   "metadata": {},
   "source": [
    "#### 단순하게 파일을 불러와서 사용하는 경우 _ 실제 사용하는 데이터가 없기 때문에 실행 시 오류  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f163cb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "data = pd.read_csv('data_path') #예제라 실제 데이터는 존재하지 않음.\n",
    "\n",
    "#CSV파일의 x칼럼의 값을 넘파이 배열로 받아 Tensor로 바꾸어 준다. \n",
    "x = torch.from_numy(data['x'].values).unsqueeze(dim=1).float() \n",
    "y = torch.from_numy(data['y'].values).unsqueeze(dim=1).float() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7d78cb",
   "metadata": {},
   "source": [
    "#### 커스텀 데이터셋을 만들어서 사용하는 경우 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "535a7bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1953bf8f9e8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader \n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,csv_file): # csv_file 파라미터를 통해 데이터셋을 불러온다. \n",
    "        self.label = pd.read_csv(csv_file)\n",
    "        \n",
    "    def __len__(self): #전체 데이터셋의 size를 반환합니다. \n",
    "        return len(self.label)\n",
    "    \n",
    "    def __getitem__(self,idx): #전체 x와 y데이터 중에 해당 idx번째의 데이터를 가져옵니다. \n",
    "        sample = torch.tensor(self.label.iloc[idx,0:3]).int()\n",
    "        label = torch.tensor(self.label.iloc[idx,3]).int()\n",
    "        \n",
    "        return sample, label\n",
    "    \n",
    "tensor_dataset = CustomDataset('C:/Users/CoIn241/Desktop/chan/torchbook_data/chap02/data/car_evaluation.csv')\n",
    "dataset = DataLoader(tensor_dataset,batch_size=4,shuffle = True) \n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4392a0d",
   "metadata": {},
   "source": [
    "#### 파이토치에서 제공하는 데이터셋을 사용하는 경우 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82b1dbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST 데이터셋 다운 예제 \n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "mnist_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,),(1.0)) #평균이 0.5 표준편차가 1.0이 되도록 데이터 분포를 조정\n",
    "])\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "import requests \n",
    "\n",
    "download_root = 'donwload_path' #다운받을 경로 지정 \n",
    "\n",
    "train_dataset = MNIST(download_root,transform=mnist_transform,train=True,downlaod=True)\n",
    "valid_dataset = MNIST(download_root,transform=mnist_transform,train=False,downlaod=True)\n",
    "test_dataset = MNIST(download_root,transform=mnist_transform,train=False,downlaod=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e218ded",
   "metadata": {},
   "source": [
    "### 모델 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eadf276",
   "metadata": {},
   "source": [
    "#### 단순 신경망을 정의하는 방법 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "421c3411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "model = nn.Linear(in_features =1, out_features=1, bias = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969cf8d4",
   "metadata": {},
   "source": [
    "#### nn.Module()을 상속하여 정의하는 방법 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d239f4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,inputs):\n",
    "        super(MLP,self).__init__()\n",
    "        self.layer = Linear(inputs,1)  # 계층 정의 \n",
    "        self.activation = Sigmoid() # 활성화 함수 정의\n",
    "\n",
    "    def forward(self,X):\n",
    "        X = self.layer(X)\n",
    "        X = self.activation(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aff339a",
   "metadata": {},
   "source": [
    "#### Sequential 신경망을 정의하는 방법 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d10bc54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing children\n",
      "[Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "), Sequential(\n",
      "  (0): Conv2d(64, 30, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "), Sequential(\n",
      "  (0): Linear(in_features=750, out_features=10, bias=True)\n",
      "  (1): ReLU(inplace=True)\n",
      ")]\n",
      "printing Modules\n",
      "[MLP(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(64, 30, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Linear(in_features=750, out_features=10, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "), Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "), Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1)), ReLU(inplace=True), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Sequential(\n",
      "  (0): Conv2d(64, 30, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "), Conv2d(64, 30, kernel_size=(5, 5), stride=(1, 1)), ReLU(inplace=True), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Sequential(\n",
      "  (0): Linear(in_features=750, out_features=10, bias=True)\n",
      "  (1): ReLU(inplace=True)\n",
      "), Linear(in_features=750, out_features=10, bias=True), ReLU(inplace=True)]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP,self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3,out_channels=64,kernel_size=5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2))\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64,out_channels=30,kernel_size=5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2))\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(in_features=30*5*5,out_features=10,bias=True),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = x.view(x.shape[0],-1)\n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = MLP() # 모델에 대한 객체 생성\n",
    "\n",
    "print(\"Printing children\")\n",
    "print(list(model.children())) #같은 수준의 하위 노드를 반환  :::: Sequential 밑에 layer 인데, layer 수준만 \n",
    "print(\"printing Modules\")\n",
    "print(list(model.modules())) # 모델 네트워크의 모든 노드를 반환 :::: Sequential 밑에 layer인데, 모두 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a76b5c",
   "metadata": {},
   "source": [
    "#### 함수로 신경망을 정의하는 방법 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ffbde6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP(in_features=1,hidden_features=20,out_featrues=1):\n",
    "    hidden = nn.Linear() #인자 지정해줘야 함 오류 발생\n",
    "    activation = nn.ReLU()\n",
    "    output = nn.Linear()\n",
    "    net = nn.Sequential(hidden, activation, output)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c6a103",
   "metadata": {},
   "source": [
    "### 2.2.4 모델의 파라미터 정의 \n",
    "- 손실함수\n",
    "- 옵티마이저\n",
    "- 스케줄러 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "303f4dc9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6304\\509492238.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmomentum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer=optimizer,\n\u001b[0;32m      8\u001b[0m                                              lr_lambda=lambda epoch:0.95 ** epoch)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# 예시 코드 (형식만 확인) \n",
    "import torch\n",
    "from torch.optim import optimizer \n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.01,momentum = 0.9)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer=optimizer,\n",
    "                                             lr_lambda=lambda epoch:0.95 ** epoch)\n",
    "\n",
    "for epoch in range(1,100+1):\n",
    "    for x, y in dataloader:\n",
    "        optimzier.zero_grad()\n",
    "loss_fn(model(x),y).backward()\n",
    "optimizer.step()\n",
    "scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd34921",
   "metadata": {},
   "source": [
    "### 2.2.5 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac38571d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6304\\1412728298.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0myhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    \n",
    "    yhat = model(x_train)\n",
    "    loss = criterion(yhat,y_train)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b78a8a4",
   "metadata": {},
   "source": [
    "### 2.2.6 모델 평가\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f24fe87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3000)\n"
     ]
    }
   ],
   "source": [
    "# 함수를 이용하여 평가하는 코드 \n",
    "import torch\n",
    "import torchmetrics\n",
    "\n",
    "preds = torch.randn(10,5).softmax(dim=-1)\n",
    "target = torch.randint(5,(10,))\n",
    "\n",
    "acc = torchmetrics.functional.accuracy(preds,target)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eff09a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "tensor(0.2000)\n",
      "tensor(0.1000)\n",
      "tensor(0.3000)\n",
      "tensor(0.1000)\n",
      "tensor(0.2000)\n",
      "tensor(0.2000)\n",
      "tensor(0.1000)\n",
      "tensor(0.2000)\n",
      "tensor(0.2000)\n",
      "---------\n",
      "tensor(0.1600)\n"
     ]
    }
   ],
   "source": [
    "# 모듈을 이용하여 모델을 평가하는 코드 \n",
    "import torch\n",
    "import torchmetrics\n",
    "\n",
    "metric = torchmetrics.Accuracy()\n",
    "\n",
    "n_batchs = 10\n",
    "for i in range(n_batchs):\n",
    "    preds = torch.randn(10,5).softmax(dim=-1)\n",
    "    target = torch.randint(5,(10,))\n",
    "    \n",
    "    acc = metric(preds,target) #각 배치에서의 acc\n",
    "    print(acc)\n",
    "acc = metric.compute() # 모든 배치 acc \n",
    "print(\"---------\")\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7ce62c",
   "metadata": {},
   "source": [
    "### 2.2.7 훈련 과정 모니터링 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2da9e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# writer = SummaryWriter(\"save path\")\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train() #학습 모드로 전환(dropout = True)\n",
    "#     batch_loss = 0.0\n",
    "    \n",
    "#     for i,(x,y) in enumerate(dataloader):\n",
    "#         x,y = x.to(device).float(), y.to(device).float()\n",
    "#         outputs = model(x)\n",
    "#         loss = criterion(outputs,y)\n",
    "#         writer.add_scalar(\"loss\",loss,epoch) # 스칼라 값을 기록\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "# writer.close() # 서머리가 더 이상 필요하지 않으면 close() 메서드 호출 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82335eab",
   "metadata": {},
   "source": [
    "## 2.4 파이토치 코드 맛보기 \n",
    "### 예제 데이터 실습 : car_evaluation.csv 데이터 파일 \n",
    "### 회귀 모델 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c047b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e65528a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>maint</th>\n",
       "      <th>doors</th>\n",
       "      <th>persons</th>\n",
       "      <th>lug_capacity</th>\n",
       "      <th>safety</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>high</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>med</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>med</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   price  maint doors persons lug_capacity safety output\n",
       "0  vhigh  vhigh     2       2        small    low  unacc\n",
       "1  vhigh  vhigh     2       2        small    med  unacc\n",
       "2  vhigh  vhigh     2       2        small   high  unacc\n",
       "3  vhigh  vhigh     2       2          med    low  unacc\n",
       "4  vhigh  vhigh     2       2          med    med  unacc"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"C:/Users/CoIn241/Desktop/chan/torchbook_data/chap02/data/car_evaluation.csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "137d743b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='output'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 데이터 시각화 \n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 8\n",
    "fig_size[1] = 6\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "dataset.output.value_counts().plot(kind = 'pie',autopct='%0.05f%%',\n",
    "                                  colors=['lightblue','lightgreen','orange','pink'],explode=(0.05,0.05,0.05,0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6bbc07f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 3, 0, 0, 2, 1],\n",
       "        [3, 3, 0, 0, 2, 2],\n",
       "        [3, 3, 0, 0, 2, 0],\n",
       "        [3, 3, 0, 0, 1, 1],\n",
       "        [3, 3, 0, 0, 1, 2],\n",
       "        [3, 3, 0, 0, 1, 0],\n",
       "        [3, 3, 0, 0, 0, 1],\n",
       "        [3, 3, 0, 0, 0, 2],\n",
       "        [3, 3, 0, 0, 0, 0],\n",
       "        [3, 3, 0, 1, 2, 1]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 전처리 \n",
    "categorical_columns = ['price','maint','doors','persons','lug_capacity','safety']\n",
    "\n",
    "for category in categorical_columns:\n",
    "    dataset[category] = dataset[category].astype('category')\n",
    "\n",
    "    # 범주형 데이터를 텐서로 변환하기 위한 절차 \n",
    "    # 범주형 데이터 -> dataset[category] -> 넘파이 배열 -> tensor\n",
    "    # cat.code 로 단어를 숫자로 변환 (넘파이 배열)\n",
    "price = dataset['price'].cat.codes.values\n",
    "maint = dataset['maint'].cat.codes.values\n",
    "doors = dataset['doors'].cat.codes.values\n",
    "persons = dataset['persons'].cat.codes.values\n",
    "lug_capacity = dataset['lug_capacity'].cat.codes.values\n",
    "safety = dataset['safety'].cat.codes.values\n",
    "\n",
    "categorical_data = np.stack([price,maint,doors,persons,lug_capacity,safety],1)\n",
    "categorical_data = torch.tensor(categorical_data, dtype = torch.int64) # 넘파이 배열을 tensor로 변환\n",
    "categorical_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90e35596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1728, 6])\n",
      "torch.Size([6912])\n"
     ]
    }
   ],
   "source": [
    "#레이블 인코딩 \n",
    "outputs = pd.get_dummies(dataset.output)\n",
    "outputs = outputs.values\n",
    "outputs = torch.tensor(outputs).flatten() # 1차원 tensor로 변환\n",
    "\n",
    "print(categorical_data.shape)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ee8b285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4, 2), (4, 2), (4, 2), (3, 2), (3, 2), (3, 2)]\n"
     ]
    }
   ],
   "source": [
    "#범주형 칼럼을 N차원으로 변환 <- 워드 임베딩은 유사한 단어끼리 유사하게 인코딩되도록 표현하는 방법\n",
    "# 임베딩 차원이 높을 수록 단어 간의 세부적인 관계를 파악 가능 따라서 N차원으로 변경\n",
    "# N차원으로 만들어주기 위해서 모든 범주형 칼럼에 대해서 임베딩 ㅋ므기를 정의 보통 칼럼의 고유값 / 2 로 설정\n",
    "\n",
    "categorical_column_sizes = [len(dataset[column].cat.categories) for column in categorical_columns]\n",
    "categorical_embedding_sizes = [(col_size,min(50,(col_size+1)//2)) for col_size in categorical_column_sizes]\n",
    "\n",
    "print(categorical_embedding_sizes) #(칼럼 고유값 수 , 차원의 크기) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "577b1357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1383\n",
      "1383\n",
      "345\n",
      "345\n"
     ]
    }
   ],
   "source": [
    "#데이터셋 분리 \n",
    "total_records = 1728\n",
    "test_records = int(total_records * .2) # 전체 데이터 수 중에서 20%만 사용 \n",
    "\n",
    "categorical_train_data = categorical_data[:total_records-test_records]\n",
    "categorical_test_data = categorical_data[total_records-test_records : total_records]\n",
    "train_outputs = outputs[:total_records - test_records]\n",
    "test_outputs = outputs[total_records - test_records : total_records]\n",
    "\n",
    "print(len(categorical_train_data))\n",
    "print(len(train_outputs))\n",
    "print(len(categorical_test_data))\n",
    "print(len(test_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d8a7398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 네트워크 생성\n",
    "class Model(nn.Module): #클래스 형태로 구현되는 방법은 nn.Module을 상속 받는다. \n",
    "    def __init__(self,embedding_size,output_size,layers,p=0.4): #신경망에서 사용될 파라미터와 초기화를 위한 용도 \n",
    "        super().__init__()\n",
    "        self.all_embeddings = nn.ModuleList([nn.Embedding(ni,nf) for ni,nf in embedding_size])\n",
    "        self.embedding_dropout = nn.Dropout(p)\n",
    "        \n",
    "        all_layers = []\n",
    "        num_categorical_cols = sum((nf for ni, nf in embedding_size))\n",
    "        input_size = num_categorical_cols #입력층의 크기를 찾기 위해 범주형 칼럼 개수를 input_size변수에 저장\n",
    "        \n",
    "        for i in layers:\n",
    "            all_layers.append(nn.Linear(input_size,i))\n",
    "            all_layers.append(nn.ReLU(inplace =True))\n",
    "            all_layers.append(nn.BatchNorm1d(i))\n",
    "            all_layers.append(nn.Dropout(p))\n",
    "            input_size=i \n",
    "            \n",
    "        all_layers.append(nn.Linear(layers[-1],output_size))\n",
    "        self.layers = nn.Sequential(*all_layers) #신경망의 모든 계층이 순차적으로 실행되도록 모든 계층에 대한 목록을 시퀀셜에 전달\n",
    "        \n",
    "    def forward(self, x_categorical): #학습 데이터를 입력 받아서 연산을 진행 모델 객체를 데이터와 함께 호출하여 사용 \n",
    "        embeddings= []\n",
    "        for i,e in enumerate(self.all_embeddings):\n",
    "            embeddings.append(e(x_categorical[:,i]))\n",
    "        x = torch.cat(embeddings,1)\n",
    "        x = self.embedding_dropout(x)\n",
    "        x = self.layers(x) \n",
    "\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a78b6fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (all_embeddings): ModuleList(\n",
      "    (0): Embedding(4, 2)\n",
      "    (1): Embedding(4, 2)\n",
      "    (2): Embedding(4, 2)\n",
      "    (3): Embedding(3, 2)\n",
      "    (4): Embedding(3, 2)\n",
      "    (5): Embedding(3, 2)\n",
      "  )\n",
      "  (embedding_dropout): Dropout(p=0.4, inplace=False)\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=12, out_features=200, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.4, inplace=False)\n",
      "    (4): Linear(in_features=200, out_features=100, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): Dropout(p=0.4, inplace=False)\n",
      "    (8): Linear(in_features=100, out_features=50, bias=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): Dropout(p=0.4, inplace=False)\n",
      "    (12): Linear(in_features=50, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Model 클래스 객체 생성\n",
    "model = Model(categorical_embedding_sizes,4,[200,100,50],p=0.4)  #p : drop_out rate \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d7ab7aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 100 loss: 0.47720486\n",
      "epoch : 200 loss: 0.47832933\n",
      "epoch : 300 loss: 0.47910732\n",
      "epoch : 400 loss: 0.46648997\n",
      "epoch : 500 loss: 0.46468151\n",
      "epoch : 600 loss: 0.47595876\n",
      "epoch : 700 loss: 0.45606452\n",
      "epoch : 800 loss: 0.45327973\n",
      "epoch : 900 loss: 0.46526504\n",
      "epoch : 1000 loss: 0.46568146\n",
      "epoch : 1100 loss: 0.46152893\n",
      "epoch : 1200 loss: 0.46298605\n",
      "epoch : 1300 loss: 0.46293521\n",
      "epoch : 1400 loss: 0.48004496\n",
      "epoch : 1500 loss: 0.45591810\n",
      "epoch : 1600 loss: 0.45921946\n",
      "epoch : 1700 loss: 0.46638516\n",
      "epoch : 1800 loss: 0.44586173\n",
      "epoch : 1900 loss: 0.44437492\n",
      "epoch : 2000 loss: 0.45847625\n",
      "epoch : 2100 loss: 0.43859580\n",
      "epoch : 2200 loss: 0.45044520\n",
      "epoch : 2300 loss: 0.44896096\n",
      "epoch : 2400 loss: 0.44323283\n",
      "epoch : 2500 loss: 0.45697469\n",
      "epoch : 2600 loss: 0.46563774\n",
      "epoch : 2700 loss: 0.45757121\n",
      "epoch : 2800 loss: 0.44603160\n",
      "epoch : 2900 loss: 0.45082840\n",
      "epoch : 3000 loss: 0.45136747\n",
      "epoch : 3100 loss: 0.45613647\n",
      "epoch : 3200 loss: 0.44628102\n",
      "epoch : 3300 loss: 0.44022650\n",
      "epoch : 3400 loss: 0.46146011\n",
      "epoch : 3500 loss: 0.45773026\n",
      "epoch : 3600 loss: 0.43628615\n",
      "epoch : 3700 loss: 0.45108095\n",
      "epoch : 3800 loss: 0.44629666\n",
      "epoch : 3900 loss: 0.44477770\n",
      "epoch : 4000 loss: 0.44834119\n",
      "epoch : 4100 loss: 0.44366363\n",
      "epoch : 4200 loss: 0.44845212\n",
      "epoch : 4300 loss: 0.44108298\n",
      "epoch : 4400 loss: 0.43211544\n",
      "epoch : 4500 loss: 0.44892681\n",
      "epoch : 4600 loss: 0.44142050\n",
      "epoch : 4700 loss: 0.42426410\n",
      "epoch : 4800 loss: 0.45074704\n",
      "epoch : 4900 loss: 0.43693095\n",
      "epoch : 5000 loss: 0.43043721\n",
      "epoch : 5100 loss: 0.45217055\n",
      "epoch : 5200 loss: 0.43509346\n",
      "epoch : 5300 loss: 0.42633313\n",
      "epoch : 5400 loss: 0.42387849\n",
      "epoch : 5500 loss: 0.43104842\n",
      "epoch : 5600 loss: 0.43026602\n",
      "epoch : 5700 loss: 0.41465104\n",
      "epoch : 5800 loss: 0.43796930\n",
      "epoch : 5900 loss: 0.43651417\n",
      "epoch : 6000 loss: 0.43720007\n",
      "epoch : 6100 loss: 0.43044242\n",
      "epoch : 6200 loss: 0.43885297\n",
      "epoch : 6300 loss: 0.42950332\n",
      "epoch : 6400 loss: 0.44163084\n",
      "epoch : 6500 loss: 0.40183830\n",
      "epoch : 6600 loss: 0.44056565\n",
      "epoch : 6700 loss: 0.42654058\n",
      "epoch : 6800 loss: 0.42116320\n",
      "epoch : 6900 loss: 0.43600571\n",
      "epoch : 7000 loss: 0.41495609\n",
      "epoch : 7100 loss: 0.42509478\n",
      "epoch : 7200 loss: 0.41721970\n",
      "epoch : 7300 loss: 0.42143711\n",
      "epoch : 7400 loss: 0.42870587\n",
      "epoch : 7500 loss: 0.42729858\n",
      "epoch : 7600 loss: 0.42333820\n",
      "epoch : 7700 loss: 0.41608548\n",
      "epoch : 7800 loss: 0.42191380\n",
      "epoch : 7900 loss: 0.40855116\n",
      "epoch : 8000 loss: 0.42012957\n",
      "epoch : 8100 loss: 0.42434752\n",
      "epoch : 8200 loss: 0.43547198\n",
      "epoch : 8300 loss: 0.41211286\n",
      "epoch : 8400 loss: 0.41512397\n",
      "epoch : 8500 loss: 0.42283806\n",
      "epoch : 8600 loss: 0.42466584\n",
      "epoch : 8700 loss: 0.39792910\n",
      "epoch : 8800 loss: 0.41843867\n",
      "epoch : 8900 loss: 0.41300547\n",
      "epoch : 9000 loss: 0.40601173\n",
      "epoch : 9100 loss: 0.42355534\n",
      "epoch : 9200 loss: 0.42361867\n",
      "epoch : 9300 loss: 0.40520397\n",
      "epoch : 9400 loss: 0.41351262\n",
      "epoch : 9500 loss: 0.39192191\n",
      "epoch : 9600 loss: 0.41517588\n",
      "epoch : 9700 loss: 0.43062413\n",
      "epoch : 9800 loss: 0.42556530\n",
      "epoch : 9900 loss: 0.41825345\n",
      "epoch : 10000 loss: 0.40782174\n",
      "epoch : 10100 loss: 0.41184714\n",
      "epoch : 10200 loss: 0.42512971\n",
      "epoch : 10300 loss: 0.42299134\n",
      "epoch : 10400 loss: 0.42167053\n",
      "epoch : 10500 loss: 0.40592507\n",
      "epoch : 10600 loss: 0.39909896\n",
      "epoch : 10700 loss: 0.39881068\n",
      "epoch : 10800 loss: 0.43921477\n",
      "epoch : 10900 loss: 0.40727648\n",
      "epoch : 11000 loss: 0.39506164\n",
      "epoch : 11100 loss: 0.40781724\n",
      "epoch : 11200 loss: 0.39673272\n",
      "epoch : 11300 loss: 0.38911319\n",
      "epoch : 11400 loss: 0.42586213\n",
      "epoch : 11500 loss: 0.40156946\n",
      "epoch : 11600 loss: 0.39279646\n",
      "epoch : 11700 loss: 0.40927064\n",
      "epoch : 11800 loss: 0.42067292\n",
      "epoch : 11900 loss: 0.41859293\n",
      "epoch : 12000 loss: 0.40920347\n",
      "epoch : 12100 loss: 0.39999479\n",
      "epoch : 12200 loss: 0.39979443\n",
      "epoch : 12300 loss: 0.39681271\n",
      "epoch : 12400 loss: 0.39232859\n",
      "epoch : 12500 loss: 0.40452781\n",
      "epoch : 12600 loss: 0.39598727\n",
      "epoch : 12700 loss: 0.40517360\n",
      "epoch : 12800 loss: 0.40131477\n",
      "epoch : 12900 loss: 0.40730202\n",
      "epoch : 13000 loss: 0.40700984\n",
      "epoch : 13100 loss: 0.39845246\n",
      "epoch : 13200 loss: 0.40152222\n",
      "epoch : 13300 loss: 0.38446835\n",
      "epoch : 13400 loss: 0.40530714\n",
      "epoch : 13500 loss: 0.40204620\n",
      "epoch : 13600 loss: 0.40075609\n",
      "epoch : 13700 loss: 0.39919847\n",
      "epoch : 13800 loss: 0.39923012\n",
      "epoch : 13900 loss: 0.39391124\n",
      "epoch : 14000 loss: 0.39616653\n",
      "epoch : 14100 loss: 0.39720568\n",
      "epoch : 14200 loss: 0.37916085\n",
      "epoch : 14300 loss: 0.39840081\n",
      "epoch : 14400 loss: 0.40760022\n",
      "epoch : 14500 loss: 0.40666002\n",
      "epoch : 14600 loss: 0.38859871\n",
      "epoch : 14700 loss: 0.40016621\n",
      "epoch : 14800 loss: 0.39219275\n",
      "epoch : 14900 loss: 0.38920972\n",
      "epoch : 15000 loss: 0.39741397\n",
      "epoch : 15100 loss: 0.39802992\n",
      "epoch : 15200 loss: 0.40452945\n",
      "epoch : 15300 loss: 0.42301124\n",
      "epoch : 15400 loss: 0.38122725\n",
      "epoch : 15500 loss: 0.38742658\n",
      "epoch : 15600 loss: 0.39710054\n",
      "epoch : 15700 loss: 0.39850873\n",
      "epoch : 15800 loss: 0.40296549\n",
      "epoch : 15900 loss: 0.38718134\n",
      "epoch : 16000 loss: 0.39312807\n",
      "epoch : 16100 loss: 0.39264566\n",
      "epoch : 16200 loss: 0.42043293\n",
      "epoch : 16300 loss: 0.38400355\n",
      "epoch : 16400 loss: 0.37818235\n",
      "epoch : 16500 loss: 0.41282278\n",
      "epoch : 16600 loss: 0.39838114\n",
      "epoch : 16700 loss: 0.38900709\n",
      "epoch : 16800 loss: 0.37823915\n",
      "epoch : 16900 loss: 0.40295896\n",
      "epoch : 17000 loss: 0.37775704\n",
      "epoch : 17100 loss: 0.39183551\n",
      "epoch : 17200 loss: 0.40073499\n",
      "epoch : 17300 loss: 0.37753934\n",
      "epoch : 17400 loss: 0.38931236\n",
      "epoch : 17500 loss: 0.37549454\n",
      "epoch : 17600 loss: 0.36649117\n",
      "epoch : 17700 loss: 0.36764151\n",
      "epoch : 17800 loss: 0.39071035\n",
      "epoch : 17900 loss: 0.39657411\n",
      "epoch : 18000 loss: 0.38659400\n",
      "epoch : 18100 loss: 0.39221469\n",
      "epoch : 18200 loss: 0.38541719\n",
      "epoch : 18300 loss: 0.39829618\n",
      "epoch : 18400 loss: 0.37876388\n",
      "epoch : 18500 loss: 0.37967977\n",
      "epoch : 18600 loss: 0.39216504\n",
      "epoch : 18700 loss: 0.37812847\n",
      "epoch : 18800 loss: 0.38824674\n",
      "epoch : 18900 loss: 0.37248665\n",
      "epoch : 19000 loss: 0.40695286\n",
      "epoch : 19100 loss: 0.38138849\n",
      "epoch : 19200 loss: 0.39546824\n",
      "epoch : 19300 loss: 0.38406122\n",
      "epoch : 19400 loss: 0.38294038\n",
      "epoch : 19500 loss: 0.37584257\n",
      "epoch : 19600 loss: 0.40073392\n",
      "epoch : 19700 loss: 0.36248732\n",
      "epoch : 19800 loss: 0.39342931\n",
      "epoch : 19900 loss: 0.38068074\n",
      "epoch : 20000 loss: 0.38945177\n",
      "epoch : 20100 loss: 0.40004149\n",
      "epoch : 20200 loss: 0.38034523\n",
      "epoch : 20300 loss: 0.37164655\n",
      "epoch : 20400 loss: 0.40437680\n",
      "epoch : 20500 loss: 0.37792236\n",
      "epoch : 20600 loss: 0.38232723\n",
      "epoch : 20700 loss: 0.38295782\n",
      "epoch : 20800 loss: 0.38323572\n",
      "epoch : 20900 loss: 0.37664875\n",
      "epoch : 21000 loss: 0.38064674\n",
      "epoch : 21100 loss: 0.38587341\n",
      "epoch : 21200 loss: 0.38585290\n",
      "epoch : 21300 loss: 0.39006183\n",
      "epoch : 21400 loss: 0.36654010\n",
      "epoch : 21500 loss: 0.38451898\n",
      "epoch : 21600 loss: 0.38704750\n",
      "epoch : 21700 loss: 0.39400232\n",
      "epoch : 21800 loss: 0.40227026\n",
      "epoch : 21900 loss: 0.39583430\n",
      "epoch : 22000 loss: 0.38026837\n",
      "epoch : 22100 loss: 0.38151678\n",
      "epoch : 22200 loss: 0.38906243\n",
      "epoch : 22300 loss: 0.38968709\n",
      "epoch : 22400 loss: 0.38134328\n",
      "epoch : 22500 loss: 0.36575899\n",
      "epoch : 22600 loss: 0.38217661\n",
      "epoch : 22700 loss: 0.38555056\n",
      "epoch : 22800 loss: 0.38154507\n",
      "epoch : 22900 loss: 0.36919007\n",
      "epoch : 23000 loss: 0.38391289\n",
      "epoch : 23100 loss: 0.38337699\n",
      "epoch : 23200 loss: 0.37674931\n",
      "epoch : 23300 loss: 0.39957869\n",
      "epoch : 23400 loss: 0.38121706\n",
      "epoch : 23500 loss: 0.38655096\n",
      "epoch : 23600 loss: 0.37883255\n",
      "epoch : 23700 loss: 0.37759623\n",
      "epoch : 23800 loss: 0.38112050\n",
      "epoch : 23900 loss: 0.38206375\n",
      "epoch : 24000 loss: 0.37970611\n",
      "epoch : 24100 loss: 0.36786363\n",
      "epoch : 24200 loss: 0.37662593\n",
      "epoch : 24300 loss: 0.38295341\n",
      "epoch : 24400 loss: 0.38386983\n",
      "epoch : 24500 loss: 0.39391482\n",
      "epoch : 24600 loss: 0.37180012\n",
      "epoch : 24700 loss: 0.37356976\n",
      "epoch : 24800 loss: 0.39759678\n",
      "epoch : 24900 loss: 0.39164966\n",
      "epoch : 25000 loss: 0.37412217\n",
      "epoch : 25100 loss: 0.38680458\n",
      "epoch : 25200 loss: 0.36930487\n",
      "epoch : 25300 loss: 0.38193470\n",
      "epoch : 25400 loss: 0.37329009\n",
      "epoch : 25500 loss: 0.37871519\n",
      "epoch : 25600 loss: 0.38350460\n",
      "epoch : 25700 loss: 0.36042386\n",
      "epoch : 25800 loss: 0.37315267\n",
      "epoch : 25900 loss: 0.37270176\n",
      "epoch : 26000 loss: 0.37296784\n",
      "epoch : 26100 loss: 0.38365725\n",
      "epoch : 26200 loss: 0.34272790\n",
      "epoch : 26300 loss: 0.37691602\n",
      "epoch : 26400 loss: 0.39611420\n",
      "epoch : 26500 loss: 0.37589839\n",
      "epoch : 26600 loss: 0.36558250\n",
      "epoch : 26700 loss: 0.35788500\n",
      "epoch : 26800 loss: 0.38084790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 26900 loss: 0.34843808\n",
      "epoch : 27000 loss: 0.36578274\n",
      "epoch : 27100 loss: 0.36314023\n",
      "epoch : 27200 loss: 0.38007328\n",
      "epoch : 27300 loss: 0.36158192\n",
      "epoch : 27400 loss: 0.37955302\n",
      "epoch : 27500 loss: 0.37368488\n",
      "epoch : 27600 loss: 0.36491352\n",
      "epoch : 27700 loss: 0.37424728\n",
      "epoch : 27800 loss: 0.37691191\n",
      "epoch : 27900 loss: 0.36970836\n",
      "epoch : 28000 loss: 0.38100231\n",
      "epoch : 28100 loss: 0.39706618\n",
      "epoch : 28200 loss: 0.36391968\n",
      "epoch : 28300 loss: 0.39836416\n",
      "epoch : 28400 loss: 0.36416200\n",
      "epoch : 28500 loss: 0.36541897\n",
      "epoch : 28600 loss: 0.40569142\n",
      "epoch : 28700 loss: 0.37659454\n",
      "epoch : 28800 loss: 0.37455800\n",
      "epoch : 28900 loss: 0.38221598\n",
      "epoch : 29000 loss: 0.38031501\n",
      "epoch : 29100 loss: 0.37035877\n",
      "epoch : 29200 loss: 0.35382417\n",
      "epoch : 29300 loss: 0.38162541\n",
      "epoch : 29400 loss: 0.36688253\n",
      "epoch : 29500 loss: 0.37608874\n",
      "epoch : 29600 loss: 0.38435125\n",
      "epoch : 29700 loss: 0.38604379\n",
      "epoch : 29800 loss: 0.37357831\n",
      "epoch : 29900 loss: 0.37085816\n",
      "epoch : 30000 loss: 0.37322026\n",
      "epoch : 30100 loss: 0.37368184\n",
      "epoch : 30200 loss: 0.38040149\n",
      "epoch : 30300 loss: 0.37059474\n",
      "epoch : 30400 loss: 0.39389345\n",
      "epoch : 30500 loss: 0.37918478\n",
      "epoch : 30600 loss: 0.36672479\n",
      "epoch : 30700 loss: 0.36754295\n",
      "epoch : 30800 loss: 0.37021539\n",
      "epoch : 30900 loss: 0.36772963\n",
      "epoch : 31000 loss: 0.37852207\n",
      "epoch : 31100 loss: 0.36938724\n",
      "epoch : 31200 loss: 0.38711944\n",
      "epoch : 31300 loss: 0.36355698\n",
      "epoch : 31400 loss: 0.38591841\n",
      "epoch : 31500 loss: 0.38558921\n",
      "epoch : 31600 loss: 0.39266926\n",
      "epoch : 31700 loss: 0.35635298\n",
      "epoch : 31800 loss: 0.35395721\n",
      "epoch : 31900 loss: 0.35826951\n",
      "epoch : 32000 loss: 0.37399057\n",
      "epoch : 32100 loss: 0.36747098\n",
      "epoch : 32200 loss: 0.37499034\n",
      "epoch : 32300 loss: 0.36066377\n",
      "epoch : 32400 loss: 0.36971393\n",
      "epoch : 32500 loss: 0.37132898\n",
      "epoch : 32600 loss: 0.37811756\n",
      "epoch : 32700 loss: 0.37240595\n",
      "epoch : 32800 loss: 0.36980414\n",
      "epoch : 32900 loss: 0.36717495\n",
      "epoch : 33000 loss: 0.37227598\n",
      "epoch : 33100 loss: 0.35924789\n",
      "epoch : 33200 loss: 0.37520331\n",
      "epoch : 33300 loss: 0.38479447\n",
      "epoch : 33400 loss: 0.38750118\n",
      "epoch : 33500 loss: 0.36648032\n",
      "epoch : 33600 loss: 0.37816814\n",
      "epoch : 33700 loss: 0.35915941\n",
      "epoch : 33800 loss: 0.36376971\n",
      "epoch : 33900 loss: 0.37937444\n",
      "epoch : 34000 loss: 0.35495332\n",
      "epoch : 34100 loss: 0.38733524\n",
      "epoch : 34200 loss: 0.37862980\n",
      "epoch : 34300 loss: 0.38770056\n",
      "epoch : 34400 loss: 0.36847267\n",
      "epoch : 34500 loss: 0.36595443\n",
      "epoch : 34600 loss: 0.37469646\n",
      "epoch : 34700 loss: 0.35382178\n",
      "epoch : 34800 loss: 0.38820744\n",
      "epoch : 34900 loss: 0.36991301\n",
      "epoch : 35000 loss: 0.37150830\n",
      "epoch : 35100 loss: 0.36792326\n",
      "epoch : 35200 loss: 0.37568185\n",
      "epoch : 35300 loss: 0.36784938\n",
      "epoch : 35400 loss: 0.35900757\n",
      "epoch : 35500 loss: 0.37472200\n",
      "epoch : 35600 loss: 0.37089375\n",
      "epoch : 35700 loss: 0.38272330\n",
      "epoch : 35800 loss: 0.37034419\n",
      "epoch : 35900 loss: 0.36568367\n",
      "epoch : 36000 loss: 0.36446187\n",
      "epoch : 36100 loss: 0.36759040\n",
      "epoch : 36200 loss: 0.36050576\n",
      "epoch : 36300 loss: 0.37247899\n",
      "epoch : 36400 loss: 0.36029229\n",
      "epoch : 36500 loss: 0.37672836\n",
      "epoch : 36600 loss: 0.36699909\n",
      "epoch : 36700 loss: 0.36067063\n",
      "epoch : 36800 loss: 0.37964612\n",
      "epoch : 36900 loss: 0.36962405\n",
      "epoch : 37000 loss: 0.37086445\n",
      "epoch : 37100 loss: 0.38711280\n",
      "epoch : 37200 loss: 0.36075589\n",
      "epoch : 37300 loss: 0.36077020\n",
      "epoch : 37400 loss: 0.36654678\n",
      "epoch : 37500 loss: 0.37221137\n",
      "epoch : 37600 loss: 0.37169591\n",
      "epoch : 37700 loss: 0.36193556\n",
      "epoch : 37800 loss: 0.38595715\n",
      "epoch : 37900 loss: 0.37835324\n",
      "epoch : 38000 loss: 0.38293496\n",
      "epoch : 38100 loss: 0.38085160\n",
      "epoch : 38200 loss: 0.37677488\n",
      "epoch : 38300 loss: 0.37151459\n",
      "epoch : 38400 loss: 0.38136292\n",
      "epoch : 38500 loss: 0.36498603\n",
      "epoch : 38600 loss: 0.37032250\n",
      "epoch : 38700 loss: 0.37268028\n",
      "epoch : 38800 loss: 0.39126387\n",
      "epoch : 38900 loss: 0.36982358\n",
      "epoch : 39000 loss: 0.34876248\n",
      "epoch : 39100 loss: 0.38527197\n",
      "epoch : 39200 loss: 0.34461623\n",
      "epoch : 39300 loss: 0.37651682\n",
      "epoch : 39400 loss: 0.35219955\n",
      "epoch : 39500 loss: 0.38999638\n",
      "epoch : 39600 loss: 0.37666917\n",
      "epoch : 39700 loss: 0.36506602\n",
      "epoch : 39800 loss: 0.35268754\n",
      "epoch : 39900 loss: 0.36955640\n",
      "epoch : 40000 loss: 0.36071748\n",
      "epoch : 40100 loss: 0.37621537\n",
      "epoch : 40200 loss: 0.36159235\n",
      "epoch : 40300 loss: 0.35139754\n",
      "epoch : 40400 loss: 0.35775813\n",
      "epoch : 40500 loss: 0.36440906\n",
      "epoch : 40600 loss: 0.37048686\n",
      "epoch : 40700 loss: 0.36808622\n",
      "epoch : 40800 loss: 0.36675987\n",
      "epoch : 40900 loss: 0.37146813\n",
      "epoch : 41000 loss: 0.37422362\n",
      "epoch : 41100 loss: 0.36381117\n",
      "epoch : 41200 loss: 0.37883446\n",
      "epoch : 41300 loss: 0.35981810\n",
      "epoch : 41400 loss: 0.36691976\n",
      "epoch : 41500 loss: 0.35742375\n",
      "epoch : 41600 loss: 0.35993871\n",
      "epoch : 41700 loss: 0.35253525\n",
      "epoch : 41800 loss: 0.35726517\n",
      "epoch : 41900 loss: 0.36585140\n",
      "epoch : 42000 loss: 0.36786059\n",
      "epoch : 42100 loss: 0.38725999\n",
      "epoch : 42200 loss: 0.35267416\n",
      "epoch : 42300 loss: 0.36735043\n",
      "epoch : 42400 loss: 0.36407399\n",
      "epoch : 42500 loss: 0.37126878\n",
      "epoch : 42600 loss: 0.38596615\n",
      "epoch : 42700 loss: 0.38671315\n",
      "epoch : 42800 loss: 0.37026447\n",
      "epoch : 42900 loss: 0.35723042\n",
      "epoch : 43000 loss: 0.34819064\n",
      "epoch : 43100 loss: 0.37291121\n",
      "epoch : 43200 loss: 0.37069425\n",
      "epoch : 43300 loss: 0.36153165\n",
      "epoch : 43400 loss: 0.37343106\n",
      "epoch : 43500 loss: 0.36401603\n",
      "epoch : 43600 loss: 0.36687145\n",
      "epoch : 43700 loss: 0.38030988\n",
      "epoch : 43800 loss: 0.37586457\n",
      "epoch : 43900 loss: 0.38331896\n",
      "epoch : 44000 loss: 0.38054577\n",
      "epoch : 44100 loss: 0.36151969\n",
      "epoch : 44200 loss: 0.35844818\n",
      "epoch : 44300 loss: 0.37386757\n",
      "epoch : 44400 loss: 0.35734716\n",
      "epoch : 44500 loss: 0.36086947\n",
      "epoch : 44600 loss: 0.35553166\n",
      "epoch : 44700 loss: 0.36956012\n",
      "epoch : 44800 loss: 0.35200113\n",
      "epoch : 44900 loss: 0.36493725\n",
      "epoch : 45000 loss: 0.36315370\n",
      "epoch : 45100 loss: 0.39754316\n",
      "epoch : 45200 loss: 0.36600128\n",
      "epoch : 45300 loss: 0.36798254\n",
      "epoch : 45400 loss: 0.35893214\n",
      "epoch : 45500 loss: 0.36984563\n",
      "epoch : 45600 loss: 0.37162903\n",
      "epoch : 45700 loss: 0.37158993\n",
      "epoch : 45800 loss: 0.36950886\n",
      "epoch : 45900 loss: 0.35236704\n",
      "epoch : 46000 loss: 0.36357450\n",
      "epoch : 46100 loss: 0.35451707\n",
      "epoch : 46200 loss: 0.36859500\n",
      "epoch : 46300 loss: 0.36065772\n",
      "epoch : 46400 loss: 0.34538504\n",
      "epoch : 46500 loss: 0.35863814\n",
      "epoch : 46600 loss: 0.34994066\n",
      "epoch : 46700 loss: 0.36365834\n",
      "epoch : 46800 loss: 0.35812142\n",
      "epoch : 46900 loss: 0.39150372\n",
      "epoch : 47000 loss: 0.35952422\n",
      "epoch : 47100 loss: 0.36695084\n",
      "epoch : 47200 loss: 0.35288745\n",
      "epoch : 47300 loss: 0.36872825\n",
      "epoch : 47400 loss: 0.36011741\n",
      "epoch : 47500 loss: 0.36339980\n",
      "epoch : 47600 loss: 0.35707641\n",
      "epoch : 47700 loss: 0.36267060\n",
      "epoch : 47800 loss: 0.33314675\n",
      "epoch : 47900 loss: 0.36226079\n",
      "epoch : 48000 loss: 0.37110972\n",
      "epoch : 48100 loss: 0.36225739\n",
      "epoch : 48200 loss: 0.36199978\n",
      "epoch : 48300 loss: 0.36928970\n",
      "epoch : 48400 loss: 0.37509289\n",
      "epoch : 48500 loss: 0.37194908\n",
      "epoch : 48600 loss: 0.35950321\n",
      "epoch : 48700 loss: 0.38268945\n",
      "epoch : 48800 loss: 0.37039182\n",
      "epoch : 48900 loss: 0.34040272\n",
      "epoch : 49000 loss: 0.36968964\n",
      "epoch : 49100 loss: 0.36486685\n",
      "epoch : 49200 loss: 0.36986354\n",
      "epoch : 49300 loss: 0.37266928\n",
      "epoch : 49400 loss: 0.36243370\n",
      "epoch : 49500 loss: 0.35187086\n",
      "epoch : 49600 loss: 0.34477898\n",
      "epoch : 49700 loss: 0.35234648\n",
      "epoch : 49800 loss: 0.35499617\n",
      "epoch : 49900 loss: 0.36462739\n",
      "epoch : 50000 loss: 0.37186244\n",
      "epoch : 50100 loss: 0.35428664\n",
      "epoch : 50200 loss: 0.37765619\n",
      "epoch : 50300 loss: 0.36217269\n",
      "epoch : 50400 loss: 0.37503412\n",
      "epoch : 50500 loss: 0.37865621\n",
      "epoch : 50600 loss: 0.37869072\n",
      "epoch : 50700 loss: 0.36657113\n",
      "epoch : 50800 loss: 0.35572508\n",
      "epoch : 50900 loss: 0.35304821\n",
      "epoch : 51000 loss: 0.37335819\n",
      "epoch : 51100 loss: 0.35991573\n",
      "epoch : 51200 loss: 0.37063438\n",
      "epoch : 51300 loss: 0.37396634\n",
      "epoch : 51400 loss: 0.35442314\n",
      "epoch : 51500 loss: 0.37384766\n",
      "epoch : 51600 loss: 0.35160100\n",
      "epoch : 51700 loss: 0.37443563\n",
      "epoch : 51800 loss: 0.35442823\n",
      "epoch : 51900 loss: 0.36071315\n",
      "epoch : 52000 loss: 0.35154551\n",
      "epoch : 52100 loss: 0.37857419\n",
      "epoch : 52200 loss: 0.35331559\n",
      "epoch : 52300 loss: 0.36002263\n",
      "epoch : 52400 loss: 0.35406750\n",
      "epoch : 52500 loss: 0.37234601\n",
      "epoch : 52600 loss: 0.36767888\n",
      "epoch : 52700 loss: 0.35853016\n",
      "epoch : 52800 loss: 0.35129386\n",
      "epoch : 52900 loss: 0.36173442\n",
      "epoch : 53000 loss: 0.37927327\n",
      "epoch : 53100 loss: 0.37990123\n",
      "epoch : 53200 loss: 0.37570295\n",
      "epoch : 53300 loss: 0.35113695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 53400 loss: 0.34993678\n",
      "epoch : 53500 loss: 0.33293208\n",
      "epoch : 53600 loss: 0.34014070\n",
      "epoch : 53700 loss: 0.36174342\n",
      "epoch : 53800 loss: 0.36276624\n",
      "epoch : 53900 loss: 0.35787943\n",
      "epoch : 54000 loss: 0.36759305\n",
      "epoch : 54100 loss: 0.34805167\n",
      "epoch : 54200 loss: 0.36064547\n",
      "epoch : 54300 loss: 0.35799140\n",
      "epoch : 54400 loss: 0.36032888\n",
      "epoch : 54500 loss: 0.35861260\n",
      "epoch : 54600 loss: 0.34561756\n",
      "epoch : 54700 loss: 0.35414898\n",
      "epoch : 54800 loss: 0.36112091\n",
      "epoch : 54900 loss: 0.36295035\n",
      "epoch : 55000 loss: 0.34866861\n",
      "epoch : 55100 loss: 0.36885110\n",
      "epoch : 55200 loss: 0.35427743\n",
      "epoch : 55300 loss: 0.36953503\n",
      "epoch : 55400 loss: 0.36903790\n",
      "epoch : 55500 loss: 0.37462074\n",
      "epoch : 55600 loss: 0.37025774\n",
      "epoch : 55700 loss: 0.37526196\n",
      "epoch : 55800 loss: 0.36720571\n",
      "epoch : 55900 loss: 0.36106965\n",
      "epoch : 56000 loss: 0.36318591\n",
      "epoch : 56100 loss: 0.35264537\n",
      "epoch : 56200 loss: 0.35635930\n",
      "epoch : 56300 loss: 0.35386050\n",
      "epoch : 56400 loss: 0.37533736\n",
      "epoch : 56500 loss: 0.37375939\n",
      "epoch : 56600 loss: 0.35946003\n",
      "epoch : 56700 loss: 0.37640029\n",
      "epoch : 56800 loss: 0.36944109\n",
      "epoch : 56900 loss: 0.36770990\n",
      "epoch : 57000 loss: 0.34226611\n",
      "epoch : 57100 loss: 0.36255473\n",
      "epoch : 57200 loss: 0.37335753\n",
      "epoch : 57300 loss: 0.35749367\n",
      "epoch : 57400 loss: 0.34714311\n",
      "epoch : 57500 loss: 0.35437620\n",
      "epoch : 57600 loss: 0.35765934\n",
      "epoch : 57700 loss: 0.36299512\n",
      "epoch : 57800 loss: 0.36204597\n",
      "epoch : 57900 loss: 0.35133070\n",
      "epoch : 58000 loss: 0.37523067\n",
      "epoch : 58100 loss: 0.35302258\n",
      "epoch : 58200 loss: 0.34227851\n",
      "epoch : 58300 loss: 0.36693311\n",
      "epoch : 58400 loss: 0.36140284\n",
      "epoch : 58500 loss: 0.35976526\n",
      "epoch : 58600 loss: 0.35014451\n",
      "epoch : 58700 loss: 0.34423086\n",
      "epoch : 58800 loss: 0.36657760\n",
      "epoch : 58900 loss: 0.37257028\n",
      "epoch : 59000 loss: 0.35149437\n",
      "epoch : 59100 loss: 0.36523998\n",
      "epoch : 59200 loss: 0.34204251\n",
      "epoch : 59300 loss: 0.35558525\n",
      "epoch : 59400 loss: 0.36116242\n",
      "epoch : 59500 loss: 0.36015388\n",
      "epoch : 59600 loss: 0.33746308\n",
      "epoch : 59700 loss: 0.35616109\n",
      "epoch : 59800 loss: 0.35598743\n",
      "epoch : 59900 loss: 0.38026491\n",
      "epoch : 60000 loss: 0.36565238\n",
      "epoch : 60100 loss: 0.33783746\n",
      "epoch : 60200 loss: 0.36221570\n",
      "epoch : 60300 loss: 0.36041865\n",
      "epoch : 60400 loss: 0.37104219\n",
      "epoch : 60500 loss: 0.33576435\n",
      "epoch : 60600 loss: 0.35232177\n",
      "epoch : 60700 loss: 0.37486538\n",
      "epoch : 60800 loss: 0.37274924\n",
      "epoch : 60900 loss: 0.37139720\n",
      "epoch : 61000 loss: 0.38060531\n",
      "epoch : 61100 loss: 0.33998823\n",
      "epoch : 61200 loss: 0.34993261\n",
      "epoch : 61300 loss: 0.35828149\n",
      "epoch : 61400 loss: 0.37676424\n",
      "epoch : 61500 loss: 0.35752138\n",
      "epoch : 61600 loss: 0.36564523\n",
      "epoch : 61700 loss: 0.36214823\n",
      "epoch : 61800 loss: 0.37673140\n",
      "epoch : 61900 loss: 0.36270538\n",
      "epoch : 62000 loss: 0.36437604\n",
      "epoch : 62100 loss: 0.37007770\n",
      "epoch : 62200 loss: 0.34475082\n",
      "epoch : 62300 loss: 0.36890820\n",
      "epoch : 62400 loss: 0.35469648\n",
      "epoch : 62500 loss: 0.37086749\n",
      "epoch : 62600 loss: 0.35553119\n",
      "epoch : 62700 loss: 0.35691446\n",
      "epoch : 62800 loss: 0.37027723\n",
      "epoch : 62900 loss: 0.38301614\n",
      "epoch : 63000 loss: 0.36938286\n",
      "epoch : 63100 loss: 0.34534013\n",
      "epoch : 63200 loss: 0.36886930\n",
      "epoch : 63300 loss: 0.36086071\n",
      "epoch : 63400 loss: 0.33627933\n",
      "epoch : 63500 loss: 0.36220545\n",
      "epoch : 63600 loss: 0.35288525\n",
      "epoch : 63700 loss: 0.34586659\n",
      "epoch : 63800 loss: 0.36345717\n",
      "epoch : 63900 loss: 0.36972076\n",
      "epoch : 64000 loss: 0.34758854\n",
      "epoch : 64100 loss: 0.35384735\n",
      "epoch : 64200 loss: 0.34069744\n",
      "epoch : 64300 loss: 0.35887587\n",
      "epoch : 64400 loss: 0.37220725\n",
      "epoch : 64500 loss: 0.36274338\n",
      "epoch : 64600 loss: 0.36467284\n",
      "epoch : 64700 loss: 0.36842379\n",
      "epoch : 64800 loss: 0.36642760\n",
      "epoch : 64900 loss: 0.35562834\n",
      "epoch : 65000 loss: 0.34933788\n",
      "epoch : 65100 loss: 0.36223876\n",
      "epoch : 65200 loss: 0.34222263\n",
      "epoch : 65300 loss: 0.33967420\n",
      "epoch : 65400 loss: 0.35373658\n",
      "epoch : 65500 loss: 0.34346595\n",
      "epoch : 65600 loss: 0.34537345\n",
      "epoch : 65700 loss: 0.36059734\n",
      "epoch : 65800 loss: 0.35549608\n",
      "epoch : 65900 loss: 0.35662356\n",
      "epoch : 66000 loss: 0.34794068\n",
      "epoch : 66100 loss: 0.35452700\n",
      "epoch : 66200 loss: 0.35497150\n",
      "epoch : 66300 loss: 0.37754396\n",
      "epoch : 66400 loss: 0.38688660\n",
      "epoch : 66500 loss: 0.36182743\n",
      "epoch : 66600 loss: 0.34619671\n",
      "epoch : 66700 loss: 0.34778577\n",
      "epoch : 66800 loss: 0.32927880\n",
      "epoch : 66900 loss: 0.34648120\n",
      "epoch : 67000 loss: 0.36380452\n",
      "epoch : 67100 loss: 0.34162745\n",
      "epoch : 67200 loss: 0.34824201\n",
      "epoch : 67300 loss: 0.39538005\n",
      "epoch : 67400 loss: 0.33308497\n",
      "epoch : 67500 loss: 0.34738022\n",
      "epoch : 67600 loss: 0.35984781\n",
      "epoch : 67700 loss: 0.37766102\n",
      "epoch : 67800 loss: 0.37038255\n",
      "epoch : 67900 loss: 0.36316577\n",
      "epoch : 68000 loss: 0.37004864\n",
      "epoch : 68100 loss: 0.35765705\n",
      "epoch : 68200 loss: 0.35776567\n",
      "epoch : 68300 loss: 0.32802805\n",
      "epoch : 68400 loss: 0.34802574\n",
      "epoch : 68500 loss: 0.35589853\n",
      "epoch : 68600 loss: 0.35286516\n",
      "epoch : 68700 loss: 0.34961683\n",
      "epoch : 68800 loss: 0.35419267\n",
      "epoch : 68900 loss: 0.36230955\n",
      "epoch : 69000 loss: 0.33856890\n",
      "epoch : 69100 loss: 0.32815543\n",
      "epoch : 69200 loss: 0.34982878\n",
      "epoch : 69300 loss: 0.34959117\n",
      "epoch : 69400 loss: 0.37556177\n",
      "epoch : 69500 loss: 0.36320397\n",
      "epoch : 69600 loss: 0.36551854\n",
      "epoch : 69700 loss: 0.34778205\n",
      "epoch : 69800 loss: 0.35542619\n",
      "epoch : 69900 loss: 0.33192301\n",
      "epoch : 70000 loss: 0.36306068\n",
      "epoch : 70100 loss: 0.35595903\n",
      "epoch : 70200 loss: 0.35647801\n",
      "epoch : 70300 loss: 0.33904830\n",
      "epoch : 70400 loss: 0.33877864\n",
      "epoch : 70500 loss: 0.34694168\n",
      "epoch : 70600 loss: 0.37728086\n",
      "epoch : 70700 loss: 0.37346852\n",
      "epoch : 70800 loss: 0.37023631\n",
      "epoch : 70900 loss: 0.35468256\n",
      "epoch : 71000 loss: 0.35799316\n",
      "epoch : 71100 loss: 0.37188980\n",
      "epoch : 71200 loss: 0.36622578\n",
      "epoch : 71300 loss: 0.36894995\n",
      "epoch : 71400 loss: 0.36223561\n",
      "epoch : 71500 loss: 0.36477676\n",
      "epoch : 71600 loss: 0.34951270\n",
      "epoch : 71700 loss: 0.35032654\n",
      "epoch : 71800 loss: 0.35510623\n",
      "epoch : 71900 loss: 0.35749614\n",
      "epoch : 72000 loss: 0.34309819\n",
      "epoch : 72100 loss: 0.35524574\n",
      "epoch : 72200 loss: 0.36689317\n",
      "epoch : 72300 loss: 0.34227082\n",
      "epoch : 72400 loss: 0.34752899\n",
      "epoch : 72500 loss: 0.35914961\n",
      "epoch : 72600 loss: 0.35790282\n",
      "epoch : 72700 loss: 0.36113101\n",
      "epoch : 72800 loss: 0.35519454\n",
      "epoch : 72900 loss: 0.38059920\n",
      "epoch : 73000 loss: 0.35204995\n",
      "epoch : 73100 loss: 0.34921068\n",
      "epoch : 73200 loss: 0.35682198\n",
      "epoch : 73300 loss: 0.34919977\n",
      "epoch : 73400 loss: 0.35316727\n",
      "epoch : 73500 loss: 0.37077299\n",
      "epoch : 73600 loss: 0.34827438\n",
      "epoch : 73700 loss: 0.36424643\n",
      "epoch : 73800 loss: 0.37468639\n",
      "epoch : 73900 loss: 0.34526652\n",
      "epoch : 74000 loss: 0.34474105\n",
      "epoch : 74100 loss: 0.35508135\n",
      "epoch : 74200 loss: 0.35690826\n",
      "epoch : 74300 loss: 0.36106679\n",
      "epoch : 74400 loss: 0.35093212\n",
      "epoch : 74500 loss: 0.34596074\n",
      "epoch : 74600 loss: 0.34318182\n",
      "epoch : 74700 loss: 0.35293433\n",
      "epoch : 74800 loss: 0.35960558\n",
      "epoch : 74900 loss: 0.37292671\n",
      "epoch : 75000 loss: 0.34124547\n",
      "epoch : 75100 loss: 0.36200958\n",
      "epoch : 75200 loss: 0.35585049\n",
      "epoch : 75300 loss: 0.37639260\n",
      "epoch : 75400 loss: 0.36613047\n",
      "epoch : 75500 loss: 0.35635602\n",
      "epoch : 75600 loss: 0.35597348\n",
      "epoch : 75700 loss: 0.35594156\n",
      "epoch : 75800 loss: 0.35436529\n",
      "epoch : 75900 loss: 0.36617890\n",
      "epoch : 76000 loss: 0.35662702\n",
      "epoch : 76100 loss: 0.35614759\n",
      "epoch : 76200 loss: 0.34997809\n",
      "epoch : 76300 loss: 0.35475197\n",
      "epoch : 76400 loss: 0.34835610\n",
      "epoch : 76500 loss: 0.37598345\n",
      "epoch : 76600 loss: 0.36627319\n",
      "epoch : 76700 loss: 0.36291277\n",
      "epoch : 76800 loss: 0.34382460\n",
      "epoch : 76900 loss: 0.34879431\n",
      "epoch : 77000 loss: 0.36027855\n",
      "epoch : 77100 loss: 0.35227984\n",
      "epoch : 77200 loss: 0.35996312\n",
      "epoch : 77300 loss: 0.35308471\n",
      "epoch : 77400 loss: 0.36255276\n",
      "epoch : 77500 loss: 0.37029698\n",
      "epoch : 77600 loss: 0.35887539\n",
      "epoch : 77700 loss: 0.35211042\n",
      "epoch : 77800 loss: 0.35147780\n",
      "epoch : 77900 loss: 0.34649929\n",
      "epoch : 78000 loss: 0.36888617\n",
      "epoch : 78100 loss: 0.36540416\n",
      "epoch : 78200 loss: 0.36187527\n",
      "epoch : 78300 loss: 0.34780464\n",
      "epoch : 78400 loss: 0.34829363\n",
      "epoch : 78500 loss: 0.34926310\n",
      "epoch : 78600 loss: 0.35194230\n",
      "epoch : 78700 loss: 0.34281132\n",
      "epoch : 78800 loss: 0.33922887\n",
      "epoch : 78900 loss: 0.36278382\n",
      "epoch : 79000 loss: 0.36857405\n",
      "epoch : 79100 loss: 0.35979667\n",
      "epoch : 79200 loss: 0.37731594\n",
      "epoch : 79300 loss: 0.34535423\n",
      "epoch : 79400 loss: 0.35314980\n",
      "epoch : 79500 loss: 0.35350430\n",
      "epoch : 79600 loss: 0.35069084\n",
      "epoch : 79700 loss: 0.34119117\n",
      "epoch : 79800 loss: 0.34720975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 79900 loss: 0.36014950\n",
      "epoch : 80000 loss: 0.37579027\n",
      "epoch : 80100 loss: 0.36253968\n",
      "epoch : 80200 loss: 0.36026269\n",
      "epoch : 80300 loss: 0.36378121\n",
      "epoch : 80400 loss: 0.35948378\n",
      "epoch : 80500 loss: 0.36148193\n",
      "epoch : 80600 loss: 0.34670192\n",
      "epoch : 80700 loss: 0.36870381\n",
      "epoch : 80800 loss: 0.35152328\n",
      "epoch : 80900 loss: 0.34026459\n",
      "epoch : 81000 loss: 0.35269663\n",
      "epoch : 81100 loss: 0.34874916\n",
      "epoch : 81200 loss: 0.34314096\n",
      "epoch : 81300 loss: 0.34218216\n",
      "epoch : 81400 loss: 0.36274463\n",
      "epoch : 81500 loss: 0.36124822\n",
      "epoch : 81600 loss: 0.35534748\n",
      "epoch : 81700 loss: 0.37566146\n",
      "epoch : 81800 loss: 0.36907718\n",
      "epoch : 81900 loss: 0.35539669\n",
      "epoch : 82000 loss: 0.33896825\n",
      "epoch : 82100 loss: 0.35657054\n",
      "epoch : 82200 loss: 0.34287965\n",
      "epoch : 82300 loss: 0.35565433\n",
      "epoch : 82400 loss: 0.34714451\n",
      "epoch : 82500 loss: 0.34807175\n",
      "epoch : 82600 loss: 0.35969079\n",
      "epoch : 82700 loss: 0.35000861\n",
      "epoch : 82800 loss: 0.35782525\n",
      "epoch : 82900 loss: 0.35653964\n",
      "epoch : 83000 loss: 0.35523662\n",
      "epoch : 83100 loss: 0.34971499\n",
      "epoch : 83200 loss: 0.36616299\n",
      "epoch : 83300 loss: 0.35749808\n",
      "epoch : 83400 loss: 0.34139466\n",
      "epoch : 83500 loss: 0.35051328\n",
      "epoch : 83600 loss: 0.36188522\n",
      "epoch : 83700 loss: 0.35614797\n",
      "epoch : 83800 loss: 0.34151828\n",
      "epoch : 83900 loss: 0.36191607\n",
      "epoch : 84000 loss: 0.32842994\n",
      "epoch : 84100 loss: 0.36925784\n",
      "epoch : 84200 loss: 0.36451584\n",
      "epoch : 84300 loss: 0.35833168\n",
      "epoch : 84400 loss: 0.36862475\n",
      "epoch : 84500 loss: 0.36776379\n",
      "epoch : 84600 loss: 0.34616739\n",
      "epoch : 84700 loss: 0.35314482\n",
      "epoch : 84800 loss: 0.37214074\n",
      "epoch : 84900 loss: 0.37421224\n",
      "epoch : 85000 loss: 0.37077534\n",
      "epoch : 85100 loss: 0.36307508\n",
      "epoch : 85200 loss: 0.36516863\n",
      "epoch : 85300 loss: 0.35746649\n",
      "epoch : 85400 loss: 0.35039073\n",
      "epoch : 85500 loss: 0.37016150\n",
      "epoch : 85600 loss: 0.35791275\n",
      "epoch : 85700 loss: 0.35761094\n",
      "epoch : 85800 loss: 0.37448007\n",
      "epoch : 85900 loss: 0.34546208\n",
      "epoch : 86000 loss: 0.34474811\n",
      "epoch : 86100 loss: 0.36163387\n",
      "epoch : 86200 loss: 0.35269555\n",
      "epoch : 86300 loss: 0.35930729\n",
      "epoch : 86400 loss: 0.36366364\n",
      "epoch : 86500 loss: 0.36581773\n",
      "epoch : 86600 loss: 0.36974213\n",
      "epoch : 86700 loss: 0.35200518\n",
      "epoch : 86800 loss: 0.35850492\n",
      "epoch : 86900 loss: 0.34755215\n",
      "epoch : 87000 loss: 0.35355431\n",
      "epoch : 87100 loss: 0.34175944\n",
      "epoch : 87200 loss: 0.35164449\n",
      "epoch : 87300 loss: 0.37761441\n",
      "epoch : 87400 loss: 0.34241319\n",
      "epoch : 87500 loss: 0.32959601\n",
      "epoch : 87600 loss: 0.37500811\n",
      "epoch : 87700 loss: 0.34054300\n",
      "epoch : 87800 loss: 0.36122221\n",
      "epoch : 87900 loss: 0.36458308\n",
      "epoch : 88000 loss: 0.37061188\n",
      "epoch : 88100 loss: 0.36449274\n",
      "epoch : 88200 loss: 0.36679211\n",
      "epoch : 88300 loss: 0.35739252\n",
      "epoch : 88400 loss: 0.36734349\n",
      "epoch : 88500 loss: 0.36582506\n",
      "epoch : 88600 loss: 0.35756984\n",
      "epoch : 88700 loss: 0.35270199\n",
      "epoch : 88800 loss: 0.35081881\n",
      "epoch : 88900 loss: 0.34253392\n",
      "epoch : 89000 loss: 0.33751592\n",
      "epoch : 89100 loss: 0.35553971\n",
      "epoch : 89200 loss: 0.35532299\n",
      "epoch : 89300 loss: 0.35587287\n",
      "epoch : 89400 loss: 0.33568078\n",
      "epoch : 89500 loss: 0.35326958\n",
      "epoch : 89600 loss: 0.36132827\n",
      "epoch : 89700 loss: 0.36149070\n",
      "epoch : 89800 loss: 0.34607220\n",
      "epoch : 89900 loss: 0.36171874\n",
      "epoch : 90000 loss: 0.32974306\n",
      "epoch : 90100 loss: 0.35272762\n",
      "epoch : 90200 loss: 0.34954199\n",
      "epoch : 90300 loss: 0.35992271\n",
      "epoch : 90400 loss: 0.36204097\n",
      "epoch : 90500 loss: 0.32874501\n",
      "epoch : 90600 loss: 0.34978154\n",
      "epoch : 90700 loss: 0.33451256\n",
      "epoch : 90800 loss: 0.36222729\n",
      "epoch : 90900 loss: 0.35017636\n",
      "epoch : 91000 loss: 0.36750284\n",
      "epoch : 91100 loss: 0.35664350\n",
      "epoch : 91200 loss: 0.33450049\n",
      "epoch : 91300 loss: 0.36331299\n",
      "epoch : 91400 loss: 0.33858123\n",
      "epoch : 91500 loss: 0.36689290\n",
      "epoch : 91600 loss: 0.36908275\n",
      "epoch : 91700 loss: 0.35336632\n",
      "epoch : 91800 loss: 0.36407703\n",
      "epoch : 91900 loss: 0.35586858\n",
      "epoch : 92000 loss: 0.34708300\n",
      "epoch : 92100 loss: 0.35840681\n",
      "epoch : 92200 loss: 0.36141333\n",
      "epoch : 92300 loss: 0.35985687\n",
      "epoch : 92400 loss: 0.37161335\n",
      "epoch : 92500 loss: 0.35189611\n",
      "epoch : 92600 loss: 0.33520097\n",
      "epoch : 92700 loss: 0.37219468\n",
      "epoch : 92800 loss: 0.36299771\n",
      "epoch : 92900 loss: 0.36108127\n",
      "epoch : 93000 loss: 0.35466090\n",
      "epoch : 93100 loss: 0.37440634\n",
      "epoch : 93200 loss: 0.35262674\n",
      "epoch : 93300 loss: 0.35604870\n",
      "epoch : 93400 loss: 0.35369754\n",
      "epoch : 93500 loss: 0.32626402\n",
      "epoch : 93600 loss: 0.38265795\n",
      "epoch : 93700 loss: 0.35823730\n",
      "epoch : 93800 loss: 0.34539023\n",
      "epoch : 93900 loss: 0.34982336\n",
      "epoch : 94000 loss: 0.33204478\n",
      "epoch : 94100 loss: 0.33663097\n",
      "epoch : 94200 loss: 0.35776663\n",
      "epoch : 94300 loss: 0.34877175\n",
      "epoch : 94400 loss: 0.34799540\n",
      "epoch : 94500 loss: 0.33074418\n",
      "epoch : 94600 loss: 0.34411338\n",
      "epoch : 94700 loss: 0.36896062\n",
      "epoch : 94800 loss: 0.35667717\n",
      "epoch : 94900 loss: 0.36574543\n",
      "epoch : 95000 loss: 0.36195576\n",
      "epoch : 95100 loss: 0.35315770\n",
      "epoch : 95200 loss: 0.36657485\n",
      "epoch : 95300 loss: 0.35929120\n",
      "epoch : 95400 loss: 0.33213925\n",
      "epoch : 95500 loss: 0.36316395\n",
      "epoch : 95600 loss: 0.34824157\n",
      "epoch : 95700 loss: 0.35185888\n",
      "epoch : 95800 loss: 0.32778484\n",
      "epoch : 95900 loss: 0.35631821\n",
      "epoch : 96000 loss: 0.35181239\n",
      "epoch : 96100 loss: 0.35316059\n",
      "epoch : 96200 loss: 0.34705761\n",
      "epoch : 96300 loss: 0.35970634\n",
      "epoch : 96400 loss: 0.35630354\n",
      "epoch : 96500 loss: 0.35684153\n",
      "epoch : 96600 loss: 0.34487644\n",
      "epoch : 96700 loss: 0.35627091\n",
      "epoch : 96800 loss: 0.36811641\n",
      "epoch : 96900 loss: 0.35249016\n",
      "epoch : 97000 loss: 0.37017706\n",
      "epoch : 97100 loss: 0.36545092\n",
      "epoch : 97200 loss: 0.33084309\n",
      "epoch : 97300 loss: 0.35748103\n",
      "epoch : 97400 loss: 0.33598047\n",
      "epoch : 97500 loss: 0.33916953\n",
      "epoch : 97600 loss: 0.35663453\n",
      "epoch : 97700 loss: 0.35727912\n",
      "epoch : 97800 loss: 0.35134691\n",
      "epoch : 97900 loss: 0.36076424\n",
      "epoch : 98000 loss: 0.35242656\n",
      "epoch : 98100 loss: 0.34490052\n",
      "epoch : 98200 loss: 0.35003677\n",
      "epoch : 98300 loss: 0.37027034\n",
      "epoch : 98400 loss: 0.35759151\n",
      "epoch : 98500 loss: 0.38467556\n",
      "epoch : 98600 loss: 0.36167181\n",
      "epoch : 98700 loss: 0.34932679\n",
      "epoch : 98800 loss: 0.35897529\n",
      "epoch : 98900 loss: 0.35508245\n",
      "epoch : 99000 loss: 0.36777043\n",
      "epoch : 99100 loss: 0.33994198\n",
      "epoch : 99200 loss: 0.33933192\n",
      "epoch : 99300 loss: 0.35120159\n",
      "epoch : 99400 loss: 0.36715880\n",
      "epoch : 99500 loss: 0.35295826\n",
      "epoch : 99600 loss: 0.35886252\n",
      "epoch : 99700 loss: 0.35053581\n",
      "epoch : 99800 loss: 0.35499379\n",
      "epoch : 99900 loss: 0.33437085\n",
      "epoch : 100000 loss: 0.34080955\n",
      "epoch:100000 loss: 0.3408095539\n"
     ]
    }
   ],
   "source": [
    "#모델 파라미터 정의 \n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "\n",
    "#CPU GPU 사용 지정\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "#모델 학습\n",
    "epochs = 100000\n",
    "aggregated_losses = []\n",
    "train_outputs = train_outputs.to(device=device,dtype=torch.int64)\n",
    "\n",
    "for i in range(epochs):\n",
    "    i += 1 \n",
    "    y_pred = model(categorical_train_data).to(device=device)\n",
    "    single_loss = loss_function(y_pred,train_outputs)\n",
    "    aggregated_losses.append(single_loss)\n",
    "    \n",
    "    if i%100 == 0:\n",
    "        print(f'epoch : {i:3} loss: {single_loss.item():10.8f}')\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    single_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(f'epoch:{i:3} loss: {single_loss.item():10.10f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "14dde7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.39863721\n"
     ]
    }
   ],
   "source": [
    "#테스트 데이터셋으로 모델 예측\n",
    "test_outputs = test_outputs.to(device=device,dtype = torch.int64)\n",
    "with torch.no_grad():\n",
    "    y_val = model(categorical_test_data).to(device=device,dtype = torch.float64)\n",
    "    loss = loss_function(y_val,test_outputs)\n",
    "print(f'loss: {loss:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b89a2892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0.9501,  -0.5631, -20.6406, -20.6586],\n",
      "        [  0.7808,  -0.0280, -20.5022, -20.5013],\n",
      "        [  2.5774,  -1.0863, -20.4208, -20.4257],\n",
      "        [  0.7598,   0.0279, -20.5742, -20.5753],\n",
      "        [  2.6905,  -1.3073, -21.0502, -21.0634]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#모델의 예측 확인\n",
    "print(y_val[:5].cpu())\n",
    "\n",
    "y_val = np.argmax(y_val.cpu(),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b96aaec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[247  12]\n",
      " [ 55  31]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.95      0.88       259\n",
      "           1       0.72      0.36      0.48        86\n",
      "\n",
      "    accuracy                           0.81       345\n",
      "   macro avg       0.77      0.66      0.68       345\n",
      "weighted avg       0.79      0.81      0.78       345\n",
      "\n",
      "0.8057971014492754\n"
     ]
    }
   ],
   "source": [
    "#테스트 데이터셋을 이용한 정확도 확인\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score \n",
    "\n",
    "\n",
    "print(confusion_matrix(test_outputs.cpu(),y_val.cpu()))\n",
    "print(classification_report(test_outputs.cpu(),y_val.cpu()))\n",
    "print(accuracy_score(test_outputs.cpu(),y_val.cpu()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nas_env",
   "language": "python",
   "name": "nas_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
